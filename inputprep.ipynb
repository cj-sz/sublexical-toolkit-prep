{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Prep\n",
    "\n",
    "This notebook prepares the CMU Dictionary for input into the Sublexical Toolkit for analysis.\n",
    "\n",
    "Author: Caleb Solomon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import cambridge_parser as parser\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Initialize the CMU Dictionary and trim it.\n",
    "\n",
    "The first many lines of the dictionary file are useless, containing simple text. There are also a significant number of words containing numbers, parentheses, or other features that are unnecessary for input to the sublexical toolkit. Furthermore, we want to keep only words whose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ";;; # CMUdict  --  Major Version: 0.07\n",
      ";;;\n",
      ";;; # $HeadURL$\n",
      ";;; # $Date::                                                   $:\n",
      ";;; # $Id::                                                     $:\n",
      ";;; # $Rev::                                                    $:\n",
      ";;; # $Author::                                                 $:\n",
      ";;;\n",
      ";;; #\n",
      ";;; # ========================================================================\n",
      ";;; # Copyright (C) 1993-2015 Carnegie Mellon University. All rights reserved.\n",
      ";;; #\n",
      ";;; # Redistribution and use in source and binary forms, with or without\n",
      ";;; # modification, are permitted provided that the following conditions\n",
      ";;; # are met:\n",
      ";;; #\n",
      ";;; # 1. Redistributions of source code must retain the above copyright\n",
      ";;; #    notice, this list of conditions and the following disclaimer.\n",
      ";;; #    The contents of this file are deemed to be source code.\n",
      ";;; #\n"
     ]
    }
   ],
   "source": [
    "# Display the first 50 or so lines for reference to above.\n",
    "fcmu = open('cmudict-0.7b-2024-4-6.txt')\n",
    "for line in fcmu.readlines()[:20]:\n",
    "    print(line.strip())\n",
    "fcmu.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block takes ~1 min to run.\n",
    "# Create a dictionary of words to pronunciations.\n",
    "# Dict {str : str}\n",
    "cmu_dict = {}\n",
    "\n",
    "# Import the SUBTLEXUS csv to a pandas dataframe.\n",
    "subtlexus = pd.read_csv('SUBTLEXusExcel2007.csv')\n",
    "\n",
    "# Convert all words to lowercase\n",
    "subtlexus['Word'] = subtlexus['Word'].str.lower()\n",
    "\n",
    "# Regex for finding unwanted punctuation in words (essentially any non-word)\n",
    "rpunc = r\".*(\\W|\\d).*\"\n",
    "# Regex for three-peated characters (any word with three or more of the same\n",
    "# letter in a row should be omitted, as none are valid English words for the\n",
    "# purposes of the toolkit)\n",
    "rpeat = r\".*(.)\\1\\1.*\"\n",
    "\n",
    "# Iterate through the lines of the dictionary. Add only such words containing\n",
    "# no parentheses and with a corresponding entry in the SUBTLEXUS to the\n",
    "# dictionary of cmu words that will be kept for analysis.\n",
    "with open('cmudict-0.7b-2024-4-6.txt') as file:\n",
    "    # Skip the first 56 lines as these contain text we are not interested in\n",
    "    for line in file.readlines()[56:]:\n",
    "        word, pronunciation = line.strip().split(maxsplit=1)\n",
    "        word = word.lower()\n",
    "        # Ensure the word doesn't contain punctuation and is present in the\n",
    "        # SUBTLEXUS\n",
    "        if re.match(rpunc, word) is None \\\n",
    "            and re.match(rpeat, word) is None \\\n",
    "            and word in subtlexus['Word'].values:\n",
    "            cmu_dict[word] = pronunciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48353"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the final number of words in the dataset\n",
    "len(cmu_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Cross-reference CMU Dictionary Pronunciations with Cambridge Prounciations\n",
    "\n",
    "First, the CMU dictionary pronunciations will need to be converted to reflect the Cambridge dictionary pronunciation format. The transcriptions csv aids in these conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a list of all possible transcriptions of a cmu word in IPA\n",
    "# form recursively.\n",
    "def possible_transcriptions(cmu_word, replacements):\n",
    "    def helper(index, current_transcription):\n",
    "        # If we are at the end of the word, return the constructed result\n",
    "        if index >= len(cmu_word):\n",
    "            transcriptions.append(current_transcription)\n",
    "            return\n",
    "\n",
    "        # Grab the current phoneme by checking to see if we are at the last\n",
    "        # phoneme (end of the word) or the next whitespace\n",
    "        pend = cmu_word[index:].find(\" \")\n",
    "        if pend != -1:\n",
    "            phoneme = cmu_word[index:index + pend]\n",
    "        else:\n",
    "            phoneme = cmu_word[index:]\n",
    "            pend = len(cmu_word)\n",
    "\n",
    "        # Determine if this is a phoneme that is sensitive to the number at the\n",
    "        # end (i.e. AH0 is differentiated from AH1)\n",
    "        m = re.match(r\"(\\w+)\\d+\", phoneme)\n",
    "        \n",
    "        # If it isn't, remove the number for consideration\n",
    "        if m != None and m.group(1) != \"AH\":\n",
    "            phoneme = m.group(1)\n",
    "\n",
    "        # Recursively generate all possible combinations of phonemes\n",
    "        if phoneme in replacements:\n",
    "            for option in replacements[phoneme]:\n",
    "                helper(index + pend + 1, current_transcription + option)\n",
    "        else:\n",
    "            helper(index + pend + 1, current_transcription + phoneme)\n",
    "\n",
    "    # Call for the word\n",
    "    transcriptions = []\n",
    "    helper(0, \"\")\n",
    "\n",
    "    # Remove all whitespace and extra numbers from the resultant transcription\n",
    "    for t in transcriptions:\n",
    "        t = t.replace(\" \", \"\")\n",
    "        t = re.sub(r\"\\d\", \"\", t)\n",
    "\n",
    "    return transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'transcriptions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the transcriptions csv\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m transcriptions \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranscriptions.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert the cmu_dict dictionary to a pandas dataframe\u001b[39;00m\n\u001b[0;32m      5\u001b[0m cmu_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mlist\u001b[39m(cmu_dict\u001b[38;5;241m.\u001b[39mitems()), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPronunciation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'transcriptions.csv'"
     ]
    }
   ],
   "source": [
    "# Load the transcriptions csv\n",
    "transcriptions = pd.read_csv('transcriptions/transcriptions.csv')\n",
    "\n",
    "# Convert the cmu_dict dictionary to a pandas dataframe\n",
    "cmu_df = pd.DataFrame(list(cmu_dict.items()), columns=['Word', 'Pronunciation'])\n",
    "\n",
    "# Iterate through the transcriptions and generate a dict of transcriptions\n",
    "# There are two special cases: ER and AA, where each have two different\n",
    "# representation possibilities. These cases need to be handled separately.\n",
    "# Furthermore, sometimes \"AA\" is followed by a number of the format \"AAn\". In\n",
    "# such cases we ignore the number and just replace as AA. To do so after we \n",
    "# apply all pronunciation transcriptions we just remove the remaining numbers\n",
    "# from the transcription. This is done below.\n",
    "replacements = {}  # Dict{str : [str]}\n",
    "special_replacements_ER = [\"ɝ\", \"ɚ\"]\n",
    "special_replacements_AA = [\"ɑ\", \"ɒ\"]\n",
    "\n",
    "for index, row in transcriptions.iterrows():\n",
    "    cmu_p = row['CMU']\n",
    "    ipa_p = row['IPA']\n",
    "\n",
    "    # Skip the special cases where the CMU pronunciation is \"ER\" or \"AA\"\n",
    "    if cmu_p == \"ER\" or cmu_p == \"AA\":\n",
    "        continue\n",
    "    \n",
    "    # Add the IPA representation transcription to the dictionary\n",
    "    replacements[cmu_p] = [ipa_p]\n",
    "\n",
    "replacements[\"ER\"] = special_replacements_ER\n",
    "replacements[\"AA\"] = special_replacements_AA\n",
    "\n",
    "# Iterate through the cmu_dict dictionary and replace all CMU pronunciations\n",
    "# with a list of all possible corresponding pronunciation transcriptions in\n",
    "# IPA.\n",
    "for index, row in cmu_df.iterrows():\n",
    "    ts = possible_transcriptions(row['Pronunciation'], replacements)\n",
    "\n",
    "    cmu_df.at[index, 'Pronunciation'] = ts\n",
    "\n",
    "# Observe some of the results\n",
    "print(cmu_df[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all CMU dictionary pronunciations have been updated to IPA format, we go through the pronunciations obtained from the Cambridge dictionary and compare.\n",
    "\n",
    "First, we write an output file with all of the words in the trimmed CMU dictionary that are not present in the Cambridge dictionary.\n",
    "\n",
    "Then, we load in the words obtained from the Cambridge dictionary and iterate through those, checking against the pronunciation from the CMU dictionary. If there is a corresponding pronunciation in the list of potential Cambridge pronunciations for a given CMU word, we take note of which one it is and mark its number in the list of pronunciations for that word (this will be written to an output file). If there is no corresponding pronunciation in the Cambridge dictionary (i.e. something is potentially amiss with the CMU dictionary pronunciation), we note \"\" as the corresponding pronunciation and indicate a \"0\" for the corresponding pronunciation. In this way we can then keep track of discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the file, cambridge_ipas.csv, containing all Cambridge words and their potential (space-separated) pronunciations.\n",
    "df = pd.read_csv(\"cambridge_ipas.csv\")\n",
    "\n",
    "cambridge_pronunciations = dict(zip(df['Word'], df['Pronunciation']))\n",
    "\n",
    "# Convert all of the pronunciations, which are currently space-separated words, into a list of such words\n",
    "for w, p in cambridge_pronunciations.items():\n",
    "    cambridge_pronunciations[w] = p.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the keys of the cambridge_pronunciations and the cmu entries to see what is missing\n",
    "cmu_dict = dict(zip(cmu_df['Word'], cmu_df['Pronunciation']))\n",
    "\n",
    "missing_dict = {}\n",
    "pop_list = []\n",
    "\n",
    "for w, p in cmu_dict.items():\n",
    "    if w not in cambridge_pronunciations.keys():\n",
    "        missing_dict[w] = ' '.join(p)\n",
    "        pop_list.append(w)\n",
    "\n",
    "final_missing = []\n",
    "\n",
    "for w, p in missing_dict.items():\n",
    "    final_missing.append((w,p))\n",
    "\n",
    "# Remove all of the words not present in Cambridge from the cmu dict list\n",
    "for word in pop_list:\n",
    "    cmu_dict.pop(word)\n",
    "\n",
    "# Output the entries in the trimmed cmu set that are not in the cambridge pronunciation set to a csv.\n",
    "df = pd.DataFrame(final_missing, columns = ['Word', 'Possible Pronunciations'])\n",
    "df.to_csv(\"missing_cmu_words.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by inspection, a lot of these words don't show up (i.e. words like \"abandoning\") because a search for said word yields the root word (i.e. searching Cambridge for \"abandoning\" yields \"abandon\"). Consequently it may be necessary to go through manually and adjust words like this with said conjugates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we iterate over the words in the CMU and Cambridge dictionaries, checking to see if any\n",
    "# possible pronunciations match. If one does, we take the matching pronunciation from the\n",
    "# Cambridge dictionary, note its number, and append the word, pronunciation, number set to a data frame.\n",
    "# If such a pronunciation does not exist, we append word, \"\", and 0 to the data frame.\n",
    "# This can later be examined to see which CMU words have discrepancies.\n",
    "df = pd.DataFrame(columns = ['Word', 'IPA Pronunciation', 'Cambridge Pronunciation Number'])\n",
    "\n",
    "for cmu_word in cmu_dict.keys():\n",
    "    # Keep track of the corresponding Cambridge pronunciation that is the same and\n",
    "    # its index, if there is one\n",
    "    corresp_p = \"\"\n",
    "    final_index = 0\n",
    "    for p in cmu_dict[cmu_word]:\n",
    "        cambridge_p_index = 0\n",
    "        for camp in cambridge_pronunciations[cmu_word]:\n",
    "            if p == camp and corresp_p == \"\":\n",
    "                # If we haven't yet found a pronunciation and an idex,\n",
    "                # and we just found one, keep track of it and we are done\n",
    "                corresp_p = camp \n",
    "                final_index = cambridge_p_index + 1\n",
    "            cambridge_p_index += 1\n",
    "\n",
    "    # Add whatever results to the data frame\n",
    "    new_row = {'Word': cmu_word, 'IPA Pronunciation': corresp_p, 'Cambridge Pronunciation Number': final_index}\n",
    "    df.loc[len(df)] = new_row\n",
    "\n",
    "# Now, for all words in the result, we want to get the Toolkit transcription as well\n",
    "tdf = pd.read_csv(\"transcriptions/transcriptions.csv\")\n",
    "t_dict = dict(zip(tdf['IPA'], tdf['Toolkit']))\n",
    "t_dict.pop(\"ɝ or ɚ\")\n",
    "t_dict.pop(\"ɑ~ɒ\")\n",
    "t_dict[\"ɑ\"] = \"a\"\n",
    "t_dict[\"ɒ\"] = \"a\"\n",
    "t_dict[\"ɚ\"] = \"3r\"\n",
    "t_dict[\"ɝ\"] = \"3r\"\n",
    "\n",
    "df['Toolkit'] = df['IPA Pronunciation'].apply(lambda x: ''.join(t_dict.get(char, char) for char in x))\n",
    "\n",
    "# Output to a csv\n",
    "df.to_csv('cmu_ipa_cambridge.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also print the number of discrepancies present\n",
    "print(f\"Number of discrepancies: {(df['Cambridge Pronunciation Number'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at accepting as an example\n",
    "print(cmu_dict[\"accepting\"])\n",
    "print(cambridge_pronunciations[\"accepting\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
